{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da21eadf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <em>Coursework 2</em>\n",
    "# Business start-ups : A Big data analysis and its impact in decision making\n",
    "\n",
    "<p>As mostly explained in my topic brief for this project, we aim to identify the key factors that play a major role when analyzing budgeting planning for start-up business using big-data algorithms and at the same time able to predict the profitability of a start-up business given its feature budgets. This is achieved by loading in a sampled dataset and testing against its features and values. This process is then replicated on the actual big data via cluster-computing frameworks(such as Pyspark for this project) & a distributed file system(such as Hadoop FS) for storage of data. The exact procedures breakdowns are once again explained in my topic brief.</p>\n",
    "\n",
    "<p><strong>Objectives overall for this project is to analyze the business start-up budgets with following aims:</strong></p>\n",
    "\n",
    "* 1.) - Aim to initialize and set-up data in forms viable for meaningful analysis.\n",
    "\n",
    "* 2.) - Ensure data scalibility by utilizing Pyspark context when doing analysis\n",
    "\n",
    "* 3.) - Ensure datasets are fit for cross analysis.\n",
    "\n",
    "* 4.) - Set-up machine-learning algorithms in Pyspark context utilizing the MLLIB functions.\n",
    "\n",
    "* 5.) - Further explore results while discussing with reference to the initial targeted aim of project.\n",
    "\n",
    "* 6.) - Cross-analyse and conclude on the impact of business budgeting features to the start-ups profitability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ad95e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#initiating spark & other useful libs\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d5f58d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Importing all important functions and libraries.\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3704b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#importing visualisation\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e525be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "\n",
    "from matplotlib import rcParams\n",
    "sns.set(context='notebook', style='whitegrid', rc={'figure.figsize': (18,4)})\n",
    "rcParams['figure.figsize'] = 18,4\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3a8da8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#random seed for reproducability\n",
    "rnd_seed=23\n",
    "np.random.seed=rnd_seed\n",
    "np.random.set_state=rnd_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4faa2554",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Startup-profits\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc84a60",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-418V0EH:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Startup-profits</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x14f94f239a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a56a239",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-418V0EH:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Startup-profits</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Startup-profits>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e47e509",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark-3.2.0-bin-hadoop2.7\\python\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x14f94f23f40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01bce25a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "startup_data = '../CW2/Startups.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557c0763",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the schema, corresponding to a line in the csv data file.\n",
    "schema = StructType([\n",
    "    StructField(\"R&D Spend\", DoubleType(), nullable=True),\n",
    "    StructField(\"Administration\", DoubleType(), nullable=True),\n",
    "    StructField(\"Marketing Spend\", DoubleType(), nullable=True),\n",
    "    StructField(\"State\", StringType(), nullable=True),\n",
    "    StructField(\"Profit\", DoubleType(), nullable=True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc4b3d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "startup_df = spark.read.option(\"header\",\"True\").csv(path=startup_data, schema=schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d6c393a",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(R&D Spend=165349.2, Administration=136897.8, Marketing Spend=471784.1, State='New York', Profit=192261.83),\n",
       " Row(R&D Spend=162597.7, Administration=151377.59, Marketing Spend=443898.53, State='California', Profit=191792.06),\n",
       " Row(R&D Spend=153441.51, Administration=101145.55, Marketing Spend=407934.54, State='Florida', Profit=191050.39),\n",
       " Row(R&D Spend=144372.41, Administration=118671.85, Marketing Spend=383199.62, State='New York', Profit=182901.99),\n",
       " Row(R&D Spend=142107.34, Administration=91391.77, Marketing Spend=366168.42, State='Florida', Profit=166187.94)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startup_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69c9b2b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R&D Spend', 'Administration', 'Marketing Spend', 'State', 'Profit']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startup_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69f2d97b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- R&D Spend: double (nullable = true)\n",
      " |-- Administration: double (nullable = true)\n",
      " |-- Marketing Spend: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startup_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce7b109",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data exploration\n",
    "\n",
    "Before making any analysis on a dataset, it is important to understand the dataset itself first. By taking a deeper look into the dataset using different spark operations such as .select & .show, we can further break the data into bite sizes to better understanding the data we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74125fa8",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|Marketing Spend|   Profit|\n",
      "+---------------+---------+\n",
      "|       471784.1|192261.83|\n",
      "|      443898.53|191792.06|\n",
      "|      407934.54|191050.39|\n",
      "|      383199.62|182901.99|\n",
      "|      366168.42|166187.94|\n",
      "|      362861.36|156991.12|\n",
      "|      127716.82|156122.51|\n",
      "|      323876.68| 155752.6|\n",
      "|      311613.29|152211.77|\n",
      "|      304981.62|149759.96|\n",
      "+---------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startup_df.select('Marketing Spend','Profit').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c33e93f",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[R&D Spend: double, Administration: double, Marketing Spend: double, State: string, Profit: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping all rows with NULL values.\n",
    "\n",
    "startup_df.na.drop(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ccdb5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+-----+------+\n",
      "|R&D Spend|Administration|Marketing Spend|State|Profit|\n",
      "+---------+--------------+---------------+-----+------+\n",
      "+---------+--------------+---------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startup_df.filter(col(\"Marketing Spend\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "238121eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+-----+------+\n",
      "|R&D Spend|Administration|Marketing Spend|State|Profit|\n",
      "+---------+--------------+---------------+-----+------+\n",
      "+---------+--------------+---------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startup_df.filter(col(\"Administration\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "690e8d97",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+-----+------+\n",
      "|R&D Spend|Administration|Marketing Spend|State|Profit|\n",
      "+---------+--------------+---------------+-----+------+\n",
      "+---------+--------------+---------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startup_df.filter(col(\"R&D Spend\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3181c3d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Brief look on other fields comparing to the target variable.\n",
    "\n",
    "group_df = startup_df.select(\"R&D Spend\",\"Profit\").sort(\"R&D Spend\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e3ef5e1",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|R&D Spend|            Profit|\n",
      "+---------+------------------+\n",
      "| 165349.2|         19.226183|\n",
      "| 162597.7|         19.179206|\n",
      "|153441.51|         19.105039|\n",
      "|144372.41|18.290198999999998|\n",
      "|142107.34|         16.618794|\n",
      "|134615.46|         15.612251|\n",
      "| 131876.9|         15.699112|\n",
      "|130298.13|          15.57526|\n",
      "|123334.88|14.975995999999999|\n",
      "|120542.52|15.221176999999999|\n",
      "+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6743cad2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Correlations of features.\n",
    "\n",
    "Below lines of codes will attempt to understand how related are the features to the targeted variable which is \"Profit\" in our current case. The higher the correlation factor, the more related the feature will be to the target variable. This simply means that in terms of statistical analysis, the higher correlational feature is the most important ones that will affect the targeted variable the most. \n",
    "\n",
    "In this case, as we can see below, R&D spend seems to drive our profits the most as it has the highest correlational value. We will keep this in mind as we proceed to conduct our machine learning algorithm in order to predict our profits using below features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37a2cfd7",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7477657217414766"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_df.stat.corr(\"Marketing Spend\",\"Profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ece1a241",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20071656826872125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startup_df.stat.corr(\"Administration\",\"Profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b382481",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9729004656594831"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startup_df.stat.corr(\"R&D Spend\",\"Profit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca7817",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Summary Stats\n",
    "\n",
    "With the in-built functions of Spark dataframes for statistical processings, we can utilize the describe function to immediately understand specificities of the datasets. This step is also important as well as it can tell us important pointers to note for regarding to this dataset.\n",
    "Some examples are as follows:\n",
    "\n",
    "* Brief count of dataset to understand the size of the sample data.\n",
    "\n",
    "* Statistics such as Mean standard deviations so as to further identify what kind of data pre-processing is needed.\n",
    "\n",
    "* Min & Max to see the scales of the dataset to identify if standard scaling is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6747ddf8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+---------------+-----+-----------+\n",
      "|summary| R&D Spend|Administration|Marketing Spend|State|     Profit|\n",
      "+-------+----------+--------------+---------------+-----+-----------+\n",
      "|  count|      50.0|          50.0|           50.0| 50.0|       50.0|\n",
      "|   mean|73721.6156|   121344.6396|    211025.0978| null|112012.6392|\n",
      "| stddev|45902.2565|    28017.8028|    122290.3107| null| 40306.1803|\n",
      "|    min|       0.0|      51283.14|            0.0| null|    14681.4|\n",
      "|    max|  165349.2|     182645.56|       471784.1| null|  192261.83|\n",
      "+-------+----------+--------------+---------------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(startup_df.describe().select(\n",
    "                    \"summary\",\n",
    "                    F.round(\"R&D Spend\",4).alias(\"R&D Spend\"),\n",
    "                    F.round(\"Administration\",4).alias(\"Administration\"),\n",
    "                    F.round(\"Marketing Spend\",4).alias(\"Marketing Spend\"),\n",
    "                    F.round(\"State\",4).alias(\"State\"),\n",
    "                    F.round(\"Profit\",4).alias(\"Profit\"),\n",
    "                    )\n",
    "                    .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6148c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data-Preprocessing\n",
    "\n",
    "<p>With the previous data exploration we did, we know enough about the data to move on to the next step of our analysis.\n",
    "For example, by looking at the difference between the min to the max value of some features, we can clearly see the need to standardize the values using a scaler as the differences are big.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fb6997d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R&D Spend', 'Administration', 'Marketing Spend', 'State', 'Profit']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startup_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d38eff56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "startup_df = startup_df.select(\n",
    "    \"R&D Spend\",\n",
    "    \"Administration\",\n",
    "    \"Marketing Spend\",\n",
    "    \"Profit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea22788",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Scaling of the profit values to the standard scaling level.\n",
    "\n",
    "As we are going to utilize the function of a standard scaler later on below in the codes. We will be scaling the \"Profit\" levels down to a similar level as the standard scaler as well. This can be done with the operation \"withColumn\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8b60e80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Factor scaling the column \"Profit\" downwards in preparation for the standard scaler.\n",
    "\n",
    "startup_df = startup_df.withColumn(\"Profit\", col(\"Profit\")/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c79fa5e",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+------------------+\n",
      "|R&D Spend|Administration|Marketing Spend|            Profit|\n",
      "+---------+--------------+---------------+------------------+\n",
      "| 165349.2|      136897.8|       471784.1|         19.226183|\n",
      "| 162597.7|     151377.59|      443898.53|         19.179206|\n",
      "|153441.51|     101145.55|      407934.54|         19.105039|\n",
      "|144372.41|     118671.85|      383199.62|18.290198999999998|\n",
      "|142107.34|      91391.77|      366168.42|         16.618794|\n",
      "| 131876.9|      99814.71|      362861.36|         15.699112|\n",
      "|134615.46|     147198.87|      127716.82|         15.612251|\n",
      "|130298.13|     145530.06|      323876.68|          15.57526|\n",
      "|120542.52|     148718.95|      311613.29|15.221176999999999|\n",
      "|123334.88|     108679.17|      304981.62|14.975995999999999|\n",
      "+---------+--------------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startup_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29fccb62",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+---------------+------------------+\n",
      "|R&D Spend|Administration|Marketing Spend|            Profit|\n",
      "+---------+--------------+---------------+------------------+\n",
      "| 165349.2|      136897.8|       471784.1|         19.226183|\n",
      "| 162597.7|     151377.59|      443898.53|         19.179206|\n",
      "|153441.51|     101145.55|      407934.54|         19.105039|\n",
      "|144372.41|     118671.85|      383199.62|18.290198999999998|\n",
      "|142107.34|      91391.77|      366168.42|         16.618794|\n",
      "| 131876.9|      99814.71|      362861.36|         15.699112|\n",
      "|134615.46|     147198.87|      127716.82|         15.612251|\n",
      "|130298.13|     145530.06|      323876.68|          15.57526|\n",
      "|120542.52|     148718.95|      311613.29|15.221176999999999|\n",
      "|123334.88|     108679.17|      304981.62|14.975995999999999|\n",
      "+---------+--------------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startup_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd30c17d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Vector-Assembler function\n",
    "\n",
    "<p>\n",
    "    The vector assembler feature of Pyspark ML is a transformer that enables the user to transform a list of columns into a single vector column which will be used during the algorithm execution step. For the below lines, we will attempt to build a vector assembler for the above mentioned purpose.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c270d6e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Set-up features list for vector assembler.\n",
    "features = [\"R&D Spend\", \"Administration\", \"Marketing Spend\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6130a3d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Importing the vector assembler function\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#Execution of the transformation.\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "assemble_df = assembler.transform(startup_df)\n",
    "assemble_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba8a64",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Standard scaling\n",
    "\n",
    "<p>\n",
    "As explained above from the data-preprocessing stage, we can observe that the differences between features are stark. Hence a scaling is required for the features in order to have a more accurate results. This will also help with the post-analysis stage for evaluation of our model to acquire reliable metrics to evaluate our algorithm results. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84fac83c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------------------------------------------------------+\n",
      "|features                       |features_scaled                                           |\n",
      "+-------------------------------+----------------------------------------------------------+\n",
      "|[165349.2,136897.8,471784.1]   |[3.6022019977107624,4.886100498126383,3.8579025370019875] |\n",
      "|[162597.7,151377.59,443898.53] |[3.542259410769301,5.402907262966764,3.6298749047677807]  |\n",
      "|[153441.51,101145.55,407934.54]|[3.3427879533360665,3.6100457585020873,3.33578790975944]  |\n",
      "|[144372.41,118671.85,383199.62]|[3.1452137882512723,4.23558731695162,3.1335239703419373]  |\n",
      "|[142107.34,91391.77,366168.42] |[3.095868283834228,3.261917816952879,2.9942553733540604]  |\n",
      "|[131876.9,99814.71,362861.36]  |[2.87299383747791,3.5625459594773656,2.967212674873934]   |\n",
      "|[134615.46,147198.87,127716.82]|[2.9326545210666466,5.2537620913604215,1.0443739920353954]|\n",
      "|[130298.13,145530.06,323876.68]|[2.8385996677575496,5.194199604802725,2.6484247041131335] |\n",
      "|[120542.52,148718.95,311613.29]|[2.6260695930375806,5.308016167358663,2.548143742136575]  |\n",
      "|[123334.88,108679.17,304981.62]|[2.6869023322968433,3.878932653943028,2.4939148342154307] |\n",
      "+-------------------------------+----------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the `standardScaler`\n",
    "\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "scaled_df = standardScaler.fit(assemble_df).transform(assemble_df)\n",
    "scaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef8929",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train-Test split\n",
    "\n",
    "<p>\n",
    "Consistent with other machine-learning techniques, we will require a split for our data into both the \"Train set\" & \"Test-set\".\n",
    "This can be achieved by utilizing the random split operation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e0d2e31",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_set,test_set = scaled_df.randomSplit([.7,.3], seed=rnd_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a06b640d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R&D Spend',\n",
       " 'Administration',\n",
       " 'Marketing Spend',\n",
       " 'Profit',\n",
       " 'features',\n",
       " 'features_scaled']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b8bd44",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "<p>\n",
    "By utilizing the Pyspark MLLIB, we can implement a linear regression model simply by initializing the context and transform each relavant datasets to the train & test sets.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f20928fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Initialize the algorithm by calling the features and target variables.\n",
    "\n",
    "lr = (LinearRegression(featuresCol=\"features_scaled\",labelCol=\"Profit\", predictionCol=\"Profit_predict\"\n",
    "     ,\n",
    "     maxIter=10, regParam=0.3, elasticNetParam=0.8, standardization=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ab47bda",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Fitting data into the train set\n",
    "\n",
    "linearModel = lr.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64e802d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([3.188, 0.0, 0.3544])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Obtaining coefficients via the in-built MLLIB functions\n",
    "\n",
    "linearModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7051f375",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.593056147001444"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6980a5d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Showcase of coefficients in a table format\n",
    "\n",
    "coeff_df = pd.DataFrame(\n",
    "    {\"Feature\": [\"Intercept\"] + features, \"Co-efficients\": np.insert(linearModel.coefficients.toArray(), 0, linearModel.intercept\n",
    "                                                                    )})\n",
    "coeff_df = coeff_df[[\"Feature\", \"Co-efficients\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ce4a92a",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Co-efficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>5.593056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R&amp;D Spend</td>\n",
       "      <td>3.188018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Administration</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marketing Spend</td>\n",
       "      <td>0.354440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature  Co-efficients\n",
       "0        Intercept       5.593056\n",
       "1        R&D Spend       3.188018\n",
       "2   Administration       0.000000\n",
       "3  Marketing Spend       0.354440"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b109419",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict = linearModel.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f58d5bf2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictlabels = predict.select(\"Profit_predict\",\"Profit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "773e5b9e",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|    Profit_predict|            Profit|\n",
      "+------------------+------------------+\n",
      "|  5.72398352627413|           1.46814|\n",
      "|  6.54555985016247|          4.949075|\n",
      "| 7.215475446183338|          6.520033|\n",
      "| 7.663932565489306| 7.149849000000001|\n",
      "| 8.090933357647366|          7.823991|\n",
      "| 8.778243227723763|          8.100576|\n",
      "|10.162846481756457| 9.993758999999999|\n",
      "|11.166673275404722|10.873399000000001|\n",
      "|11.754697527969716|         11.847403|\n",
      "|11.351375964084175|10.855203999999999|\n",
      "| 11.77741471480051|12.699292999999999|\n",
      "| 13.33534411414415|14.612195000000002|\n",
      "|14.666856035043338|13.260264999999999|\n",
      "|14.868177624299463|15.221176999999999|\n",
      "|15.803911420567498|         15.699112|\n",
      "| 16.52402416087771|         16.618794|\n",
      "| 16.73070090516566|18.290198999999998|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictlabels.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e874e5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluation of predictions\n",
    "\n",
    "<p>\n",
    "With the in-built functions of the regression evaluators from MLLIB package, we can obtain the score of our algorithm by cross-analyzing the predicted values and the actual values. Below are some examples of the evaluators:\n",
    "    \n",
    "<b>* RMSE- Root-mean-square deviation. Describes the residual standard deviations which can be summarized as the prediction errors. Generally the smaller value, the better the predictors.</b>\n",
    "\n",
    "<b>* MAE: Mean-absolute-error. Describes the mean of errors between pairs of observation regardless of the directions.</b>\n",
    "\n",
    "<b>* R2: A statistical measure that shows the coefficiency of determination between the regression line and data that are fitted in it. It can be summarized to mean how relevant the regression prediction is against the actual data plots.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21e826d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3138196604670513\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"Profit_predict\", labelCol='Profit', metricName='rmse')\n",
    "print(\"RMSE: {0}\".format(evaluator.evaluate(predictlabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e1143fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.8691229472399623\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"Profit_predict\", labelCol='Profit', metricName='mae')\n",
    "print(\"MAE: {0}\".format(evaluator.evaluate(predictlabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a2f3084",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.910598061955934\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"Profit_predict\", labelCol='Profit', metricName='r2')\n",
    "print(\"R2: {0}\".format(evaluator.evaluate(predictlabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517c1de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  Conclusions & findings\n",
    "\n",
    "<p>As we can see from the above evaluators, our algorithm obtained fairly good results from our analysis above. The RMSE shows a small value(close to zero indicating that the deviations between predictions and actual data are good. The MAE values are looking good as well as it is fairly small. It evaluates similarly to the RMSE hence both of the error metrics shows quite some good results here. Finally, the R2 value indicates that our algorithm results here are giving very strong predictions. It is always scored between 0 to 100 and with our score of .91, it means that our algorithm has high relevance in explaining the variability around the mean. </p>\n",
    "\n",
    "* <strong>Feature budget analysis.</strong>\n",
    "\n",
    "<p>From above correlation analysis, it is clear that the R&D development budget is the most impactful feature of the dataset. Out of both the correlation calculation using MLLIB function and stat operation both indicate that R&D development seem to impact the profits of a start-up most. This is shown by high correlation values between R&D budget and profits which suggests that R&D budgets will likely impact the profitability of a start-up most. This is followed next by marketing budgets and lastly administration budgets. For the conclusion derived from this project, we can assume that for a start-up business to have highest projected profitability for its business, it would be wise to concentrate their budgets more into R&D and next into Marketing.</p>\n",
    "\n",
    "* <strong>Profitability predictions</strong>\n",
    "\n",
    "<p>With the above evaluator evaluation metrics, we can assume that this algorithm has shown strong and relevant results. The metrics states that the predictions shows strong explainability to the actual data and the error rates are generally good. With this algorithm result, it is safe to say that our algorithm can predict the profitability of a start-up business on its inception planning stage to ensure a strong base before the management team can move on to its execution.</p>\n",
    "\n",
    "<p>As shown above, we have achieved our aims for this project of determining the most impactful features when considering for the budgeting input for a start-up business out of the different features. We have also managed to analyze the data to obtain a good predictor in regards to the specific sampled dataset we have. This can have very meaningful impact during a start-up planning stage as this analysis can give a yardstick measure of the profitability of the planning in regards to the amount of budgets given by the management team. It is also important to note that above project is done in Pyspark context which can easily be scaled up to very big data populations given the nature of this analysis. To get our analysis to a more reliable level, we can collect up to millions of statistics from different start-up informations to get a better understanding of the census. This would then be an issue of the codes were executed using the standard sci-kit learn. But with the Pyspark context and pairing of distributed file systems such as Hadoop, we can easily circumvent the issue and scale up using the codes as showcased in this project.</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}